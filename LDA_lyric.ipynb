{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fp = open('mxm_dataset_train.txt')\n",
    "i = 0\n",
    "texts = []\n",
    "for line in fp:\n",
    "    line = line.strip()\n",
    "    if line.startswith('#'):\n",
    "        continue\n",
    "    if line.startswith('%'):\n",
    "        wordBag = line[1:]\n",
    "        continue\n",
    "    texts.append(line.split(',')[2:]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Dai/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:16: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "/Users/Dai/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:18: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    }
   ],
   "source": [
    "words = wordBag.split(',')\n",
    "\n",
    "####### stopping\n",
    "# You can always construct your own stop word list or seek out another package to fit your use case.\n",
    "from stop_words import get_stop_words\n",
    "\n",
    "# create English stop words list\n",
    "en_stop = get_stop_words('en')\n",
    "sp_stop = get_stop_words('spanish')\n",
    "\n",
    "stop_en = []\n",
    "stop_sp = []\n",
    "\n",
    "# remove stop words from tokens\n",
    "for i in range(5000):\n",
    "    if words[i] in en_stop:\n",
    "        stop_en.append(i)\n",
    "    if words[i] in sp_stop:    \n",
    "        stop_sp.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### wordbag to dictionary\n",
    "word_dict = {}\n",
    "for i in range(len(words)):\n",
    "    if i in stop_en:\n",
    "        continue\n",
    "    if i in stop_sp:\n",
    "        continue\n",
    "    word_dict[int(i)] = words[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-547a459bb01d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mword_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "word_dict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### texts to corpus\n",
    "corpus = []\n",
    "for text in texts:\n",
    "    passage = []\n",
    "    for word in text:\n",
    "        nums = word.split(':')\n",
    "        index = int(nums[0])-1\n",
    "        count = int(nums[1])\n",
    "        if index not in stop_en and index not in stop_sp:\n",
    "            passage.append((index, count))\n",
    "    corpus.append(passage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=10, id2word = word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.029*\"get\" + 0.024*\"like\" + 0.017*\"got\" + 0.013*\"fuck\" + 0.012*\"nigga\" + 0.011*\"shit\"'),\n",
       " (1,\n",
       "  '0.013*\"god\" + 0.012*\"will\" + 0.010*\"soul\" + 0.009*\"die\" + 0.008*\"us\" + 0.008*\"one\"'),\n",
       " (2,\n",
       "  '0.042*\"know\" + 0.035*\"go\" + 0.025*\"just\" + 0.023*\"time\" + 0.021*\"let\" + 0.020*\"say\"'),\n",
       " (3,\n",
       "  '0.017*\"si\" + 0.016*\"je\" + 0.015*\"et\" + 0.011*\"amor\" + 0.010*\"il\" + 0.010*\"che\"'),\n",
       " (4,\n",
       "  '0.046*\"det\" + 0.044*\"jag\" + 0.036*\"du\" + 0.031*\"som\" + 0.031*\"och\" + 0.025*\"\\xc3\\xa4r\"'),\n",
       " (5,\n",
       "  '0.050*\"ich\" + 0.040*\"und\" + 0.038*\"die\" + 0.027*\"du\" + 0.024*\"der\" + 0.022*\"nicht\"'),\n",
       " (6,\n",
       "  '0.018*\"will\" + 0.015*\"night\" + 0.014*\"day\" + 0.013*\"come\" + 0.012*\"light\" + 0.011*\"like\"'),\n",
       " (7,\n",
       "  '0.022*\"got\" + 0.018*\"man\" + 0.016*\"hey\" + 0.014*\"like\" + 0.013*\"girl\" + 0.012*\"get\"'),\n",
       " (8,\n",
       "  '0.112*\"love\" + 0.067*\"oh\" + 0.051*\"babi\" + 0.038*\"yeah\" + 0.018*\"want\" + 0.017*\"come\"'),\n",
       " (9,\n",
       "  '0.039*\"will\" + 0.018*\"can\" + 0.017*\"never\" + 0.014*\"life\" + 0.014*\"see\" + 0.013*\"just\"')]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.print_topics(num_topics=10, num_words=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
