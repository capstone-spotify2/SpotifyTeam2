{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is functions to running hashing vectorizer on raw lyrics input and outputs a dimensionality reduced feature vectors for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2009, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>Name</th>\n",
       "      <th>Aritist</th>\n",
       "      <th>tag</th>\n",
       "      <th>Song_txt_loc</th>\n",
       "      <th>Song_lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>TRAAAED128E0783FAB</td>\n",
       "      <td>It's About Time</td>\n",
       "      <td>Jamie Cullum</td>\n",
       "      <td>relax</td>\n",
       "      <td>./lyrics_new/jamiecullum_itsabouttime.txt</td>\n",
       "      <td>Walking down to the water's edgeWhere I have ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>TRAACER128F4290F96</td>\n",
       "      <td>Setting Fire to Sleeping Giants</td>\n",
       "      <td>The Dillinger Escape Plan</td>\n",
       "      <td>energetic</td>\n",
       "      <td>./lyrics_new/dillingerescapeplan_settingfireto...</td>\n",
       "      <td>First off Let me say you look so tired... Res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>TRAADVO128E07999E9</td>\n",
       "      <td>Oh God</td>\n",
       "      <td>Jamie Cullum</td>\n",
       "      <td>relax</td>\n",
       "      <td>./lyrics_new/jamiecullum_ohgod.txt</td>\n",
       "      <td>I know it's been a while since I have talked ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>TRAAFGQ128F427D884</td>\n",
       "      <td>One Last Time</td>\n",
       "      <td>The Kooks</td>\n",
       "      <td>sad</td>\n",
       "      <td>./lyrics_new/kooks_onelasttime.txt</td>\n",
       "      <td>Can I hold you one last time To fight the fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>TRAAGCZ128F93210FD</td>\n",
       "      <td>Let's Get It Started</td>\n",
       "      <td>Black Eyed Peas</td>\n",
       "      <td>happy</td>\n",
       "      <td>./lyrics_new/blackeyedpeas_letsgetitstarted.txt</td>\n",
       "      <td>Let's get it started in here...  And the bass...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  index                  id                             Name  \\\n",
       "0           0      0  TRAAAED128E0783FAB                  It's About Time   \n",
       "1           1      3  TRAACER128F4290F96  Setting Fire to Sleeping Giants   \n",
       "2           2      7  TRAADVO128E07999E9                           Oh God   \n",
       "3           3      8  TRAAFGQ128F427D884                    One Last Time   \n",
       "4           4     10  TRAAGCZ128F93210FD             Let's Get It Started   \n",
       "\n",
       "                     Aritist        tag  \\\n",
       "0               Jamie Cullum      relax   \n",
       "1  The Dillinger Escape Plan  energetic   \n",
       "2               Jamie Cullum      relax   \n",
       "3                  The Kooks        sad   \n",
       "4            Black Eyed Peas      happy   \n",
       "\n",
       "                                        Song_txt_loc  \\\n",
       "0          ./lyrics_new/jamiecullum_itsabouttime.txt   \n",
       "1  ./lyrics_new/dillingerescapeplan_settingfireto...   \n",
       "2                 ./lyrics_new/jamiecullum_ohgod.txt   \n",
       "3                 ./lyrics_new/kooks_onelasttime.txt   \n",
       "4    ./lyrics_new/blackeyedpeas_letsgetitstarted.txt   \n",
       "\n",
       "                                         Song_lyrics  \n",
       "0   Walking down to the water's edgeWhere I have ...  \n",
       "1   First off Let me say you look so tired... Res...  \n",
       "2   I know it's been a while since I have talked ...  \n",
       "3   Can I hold you one last time To fight the fee...  \n",
       "4   Let's get it started in here...  And the bass...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Song_with_lyrics.csv\")\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'blue'>\n",
    "Note: The work of matching the lyrics with songs takes long time because the raw lyrics and song information came from different sources. Some songs might not be able to match with the lyrics due to the way that artist name has been written in difference sources. Additionally, scraping lyrics takes extremely long time, not all the songs have been scrped down. Therefore, so far we only used around 2000 data points to test if hashing vectorizer is a good way to increase accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lyr = df.Song_lyrics.values\n",
    "lyr2 = ['' if type(i) != str else i for i in lyr ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2009"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lyr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2009"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hv = HashingVectorizer(n_features=50)\n",
    "trans = hv.transform(lyr2)\n",
    "# convert to dense matrix\n",
    "dense = trans.todense()\n",
    "dense = dense.tolist()\n",
    "final_tag = df['tag'].values\n",
    "len(final_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = \"blue\">\n",
    "convert to a dataframe with response variable to a numerical value with all the features with hashing vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(dense)):\n",
    "    dense[i].append(final_tag[i])\n",
    "final_df= pd.DataFrame.from_records(dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.202241</td>\n",
       "      <td>-0.057783</td>\n",
       "      <td>0.173350</td>\n",
       "      <td>-0.028892</td>\n",
       "      <td>-0.057783</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.202241</td>\n",
       "      <td>0.173350</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086675</td>\n",
       "      <td>-0.057783</td>\n",
       "      <td>-0.028892</td>\n",
       "      <td>-0.144458</td>\n",
       "      <td>-0.751182</td>\n",
       "      <td>0.144458</td>\n",
       "      <td>-0.028892</td>\n",
       "      <td>0.057783</td>\n",
       "      <td>0.086675</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.184988</td>\n",
       "      <td>0.061663</td>\n",
       "      <td>0.061663</td>\n",
       "      <td>0.030831</td>\n",
       "      <td>-0.184988</td>\n",
       "      <td>0.030831</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.339145</td>\n",
       "      <td>-0.061663</td>\n",
       "      <td>...</td>\n",
       "      <td>0.308313</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.030831</td>\n",
       "      <td>-0.030831</td>\n",
       "      <td>-0.154157</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.154157</td>\n",
       "      <td>0.030831</td>\n",
       "      <td>0.030831</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.274352</td>\n",
       "      <td>0.078386</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078386</td>\n",
       "      <td>-0.039193</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.235159</td>\n",
       "      <td>0.117579</td>\n",
       "      <td>...</td>\n",
       "      <td>0.548703</td>\n",
       "      <td>-0.235159</td>\n",
       "      <td>-0.078386</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.235159</td>\n",
       "      <td>-0.195965</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.135457</td>\n",
       "      <td>0.067729</td>\n",
       "      <td>-0.135457</td>\n",
       "      <td>0.067729</td>\n",
       "      <td>-0.203186</td>\n",
       "      <td>-0.067729</td>\n",
       "      <td>0.067729</td>\n",
       "      <td>-0.067729</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.406371</td>\n",
       "      <td>-0.067729</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.203186</td>\n",
       "      <td>-0.406371</td>\n",
       "      <td>0.067729</td>\n",
       "      <td>-0.135457</td>\n",
       "      <td>-0.135457</td>\n",
       "      <td>-0.067729</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.317518</td>\n",
       "      <td>0.021168</td>\n",
       "      <td>-0.370438</td>\n",
       "      <td>0.010584</td>\n",
       "      <td>0.021168</td>\n",
       "      <td>-0.095255</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.031752</td>\n",
       "      <td>-0.095255</td>\n",
       "      <td>-0.010584</td>\n",
       "      <td>...</td>\n",
       "      <td>0.063504</td>\n",
       "      <td>-0.010584</td>\n",
       "      <td>0.031752</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.285766</td>\n",
       "      <td>0.063504</td>\n",
       "      <td>-0.021168</td>\n",
       "      <td>0.624452</td>\n",
       "      <td>0.074088</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.000000 -0.202241 -0.057783  0.173350 -0.028892 -0.057783  0.000000   \n",
       "1  0.000000  0.184988  0.061663  0.061663  0.030831 -0.184988  0.030831   \n",
       "2  0.000000  0.274352  0.078386  0.000000  0.078386 -0.039193  0.000000   \n",
       "3  0.000000  0.135457  0.067729 -0.135457  0.067729 -0.203186 -0.067729   \n",
       "4 -0.317518  0.021168 -0.370438  0.010584  0.021168 -0.095255  0.000000   \n",
       "\n",
       "         7         8         9  ...         41        42        43        44  \\\n",
       "0  0.000000 -0.202241  0.173350 ...   0.086675 -0.057783 -0.028892 -0.144458   \n",
       "1  0.000000 -0.339145 -0.061663 ...   0.308313  0.000000 -0.030831 -0.030831   \n",
       "2  0.000000 -0.235159  0.117579 ...   0.548703 -0.235159 -0.078386  0.000000   \n",
       "3  0.067729 -0.067729  0.000000 ...   0.406371 -0.067729  0.000000 -0.203186   \n",
       "4 -0.031752 -0.095255 -0.010584 ...   0.063504 -0.010584  0.031752  0.000000   \n",
       "\n",
       "         45        46        47        48        49   50  \n",
       "0 -0.751182  0.144458 -0.028892  0.057783  0.086675  4.0  \n",
       "1 -0.154157  0.000000 -0.154157  0.030831  0.030831  3.0  \n",
       "2  0.235159 -0.195965  0.000000  0.000000  0.000000  4.0  \n",
       "3 -0.406371  0.067729 -0.135457 -0.135457 -0.067729  2.0  \n",
       "4 -0.285766  0.063504 -0.021168  0.624452  0.074088  1.0  \n",
       "\n",
       "[5 rows x 51 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df[50] = final_df[50].map({'happy': 1.0, 'sad': 2.0,'energetic': 3.0, 'relax': 4.0})\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = \"blue\">\n",
    "### split the data points to train/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1088</th>\n",
       "      <td>0.021243</td>\n",
       "      <td>0.127458</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042486</td>\n",
       "      <td>-0.021243</td>\n",
       "      <td>0.021243</td>\n",
       "      <td>-0.063729</td>\n",
       "      <td>-0.021243</td>\n",
       "      <td>-0.106215</td>\n",
       "      <td>0.106215</td>\n",
       "      <td>...</td>\n",
       "      <td>0.276159</td>\n",
       "      <td>-0.106215</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.084972</td>\n",
       "      <td>0.424859</td>\n",
       "      <td>-0.063729</td>\n",
       "      <td>0.084972</td>\n",
       "      <td>0.042486</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>769</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.159719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063888</td>\n",
       "      <td>-0.031944</td>\n",
       "      <td>-0.031944</td>\n",
       "      <td>-0.063888</td>\n",
       "      <td>-0.031944</td>\n",
       "      <td>-0.031944</td>\n",
       "      <td>...</td>\n",
       "      <td>0.383326</td>\n",
       "      <td>0.351382</td>\n",
       "      <td>-0.127775</td>\n",
       "      <td>0.031944</td>\n",
       "      <td>-0.319438</td>\n",
       "      <td>-0.031944</td>\n",
       "      <td>-0.159719</td>\n",
       "      <td>0.063888</td>\n",
       "      <td>0.255551</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1305</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.057831</td>\n",
       "      <td>-0.057831</td>\n",
       "      <td>0.057831</td>\n",
       "      <td>-0.057831</td>\n",
       "      <td>-0.115663</td>\n",
       "      <td>-0.057831</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.404820</td>\n",
       "      <td>-0.057831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.231326</td>\n",
       "      <td>0.057831</td>\n",
       "      <td>-0.057831</td>\n",
       "      <td>-0.057831</td>\n",
       "      <td>-0.289157</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.057831</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>0.041065</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.082130</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082130</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.041065</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.287456</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.164260</td>\n",
       "      <td>0.123195</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.041065</td>\n",
       "      <td>0.041065</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.137073</td>\n",
       "      <td>-0.091382</td>\n",
       "      <td>0.045691</td>\n",
       "      <td>-0.045691</td>\n",
       "      <td>-0.045691</td>\n",
       "      <td>-0.045691</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.319838</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.137073</td>\n",
       "      <td>0.045691</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.091382</td>\n",
       "      <td>0.091382</td>\n",
       "      <td>0.091382</td>\n",
       "      <td>0.045691</td>\n",
       "      <td>-0.045691</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6   \\\n",
       "1088  0.021243  0.127458  0.000000  0.042486 -0.021243  0.021243 -0.063729   \n",
       "769   0.000000 -0.159719  0.000000  0.000000  0.063888 -0.031944 -0.031944   \n",
       "1305  0.000000  0.057831 -0.057831  0.057831 -0.057831 -0.115663 -0.057831   \n",
       "436   0.041065  0.000000 -0.082130  0.000000  0.082130  0.000000  0.000000   \n",
       "924   0.000000  0.137073 -0.091382  0.045691 -0.045691 -0.045691 -0.045691   \n",
       "\n",
       "            7         8         9  ...         41        42        43  \\\n",
       "1088 -0.021243 -0.106215  0.106215 ...   0.276159 -0.106215  0.000000   \n",
       "769  -0.063888 -0.031944 -0.031944 ...   0.383326  0.351382 -0.127775   \n",
       "1305  0.000000 -0.404820 -0.057831 ...   0.231326  0.057831 -0.057831   \n",
       "436   0.000000 -0.041065  0.000000 ...   0.287456  0.000000  0.000000   \n",
       "924   0.000000 -0.319838  0.000000 ...   0.137073  0.045691  0.000000   \n",
       "\n",
       "            44        45        46        47        48        49   50  \n",
       "1088  0.000000 -0.084972  0.424859 -0.063729  0.084972  0.042486  1.0  \n",
       "769   0.031944 -0.319438 -0.031944 -0.159719  0.063888  0.255551  2.0  \n",
       "1305 -0.057831 -0.289157  0.000000  0.000000 -0.057831  0.000000  1.0  \n",
       "436   0.000000 -0.164260  0.123195  0.000000 -0.041065  0.041065  2.0  \n",
       "924   0.000000 -0.091382  0.091382  0.091382  0.045691 -0.045691  1.0  \n",
       "\n",
       "[5 rows x 51 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msk = np.random.randint(len(final_df),size = len(final_df))\n",
    "train_index = msk[:int(0.75*len(final_df))]\n",
    "test_index = msk[int(0.75*len(final_df)):]\n",
    "train = final_df.iloc[train_index]\n",
    "test = final_df.iloc[test_index]\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x = train.values[:,:-1]\n",
    "train_y = train.values[:,-1]\n",
    "test_x = test.values[:,:-1]\n",
    "test_y = test.values[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = \"blue\">\n",
    "### apply basic classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of SVM with HashingVectorizer is: 0.419483101392\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC()\n",
    "clf.fit(train_x, train_y)  \n",
    "y_hat = clf.predict(test_x)\n",
    "score = clf.score(test_x,test_y)\n",
    "print \"The accuracy of SVM with HashingVectorizer is: \"+ str(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of Random Forest with HashingVectorizer is: 0.479125248509\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RF_short = RandomForestClassifier(n_estimators = 15, max_depth =5)\n",
    "RF_short.fit(train_x, train_y)\n",
    "y_pred_rf_short = RF_short.predict(test_x)\n",
    "y_pred_rf_short_prob = RF_short.predict_proba(test_x)\n",
    "y_score_rf_short = RF_short.score(test_x,test_y)\n",
    "print \"The accuracy of Random Forest with HashingVectorizer is: \"+ str(y_score_rf_short)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of Gaussian NB with HashingVectorizer is: 0.403578528827\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf1 = GaussianNB()\n",
    "clf1.fit(train_x, train_y)\n",
    "nb_score = clf1.score(test_x,test_y)\n",
    "print \"The accuracy of Gaussian NB with HashingVectorizer is: \"+ str(nb_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font color = \"blue\">\n",
    "# literature Review on Hashing Vectorizer\n",
    "Among all the dimensionality reduction methods we have tried, Hashing vectorizer gives the best result. Hashingvectorizer uses the hashing trick to find the token string name to feature integer index mapping and convert a collection of text documents to a matrix of token occurrences.\n",
    "\n",
    "We started exploring this method because we want to reduce the 5000 features in bag-of-words to a low dimension. Originally, we want to use hashingvectorizer the same way as tf-idf, and PCA, where we input bag-of-words as the input. However, hashing vectorizer requires raw lyrics as inputs because hashing vectorizer will tokenize the words into different bins according to the chronological order of the words in the sentence. Using the hashing trick builded in the function, the hashing vectorizer outputs nxm feature matrix where n is number of observation, m is the number of features we want to reduce to.\n",
    "\n",
    "Using Hashingvectorizer, we are able to increase the accuracy of the prediction and gets the highest accuracy in the basic classification model compare to other dimensionality reduction methods.\n",
    "# analysis:\n",
    "from the result above, we can see that even with this small amount of data set, we are able to get the best accuracy compare to other dimensionality reduction. Among all three basic classification model, Random Forest gives us the best result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### select n_features for best result\n",
    "we do it by having a train, validation, and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1 (2009, 2) 11 (2009, 12) 21 (2009, 22) 31 (2009, 32) 41 (2009, 42) 51 (2009, 52) 61 (2009, 62) 71 (2009, 72) 81 (2009, 82) 91 (2009, 92) 101 (2009, 102) 111 (2009, 112) 121 (2009, 122) 131 (2009, 132) 141 (2009, 142) 151 (2009, 152) 161 (2009, 162) 171 (2009, 172) 181 (2009, 182) 191 (2009, 192)\n"
     ]
    }
   ],
   "source": [
    "final_tag = df['tag'].values\n",
    "msk = np.random.randint(len(final_df),size = len(final_df))\n",
    "train_index = msk[:int(0.60*len(final_df))]\n",
    "valid_index = msk[int(0.60*len(final_df)):int(0.80*len(final_df))]\n",
    "test_index = msk[int(0.80*len(final_df)):]\n",
    "y_score_rf_l = []\n",
    "\n",
    "for index in range(1,201,10):\n",
    "    print index,\n",
    "    hv = HashingVectorizer(n_features=index)\n",
    "    trans = hv.transform(lyr2)\n",
    "    # convert to dense matrix\n",
    "    dense = trans.todense()\n",
    "    dense = dense.tolist()\n",
    "    for i in range(len(dense)):\n",
    "        dense[i].append(final_tag[i])\n",
    "    final_df= pd.DataFrame.from_records(dense)\n",
    "    print(final_df.shape),\n",
    "    final_df[index] = final_df[index].map({'happy': 1.0, 'sad': 2.0,'energetic': 3.0, 'relax': 4.0})\n",
    "    # split train_test file\n",
    "    train = final_df.iloc[train_index]\n",
    "    valid = final_df.iloc[valid_index]\n",
    "    test = final_df.iloc[test_index]\n",
    "    train_x = train.values[:,:-1]\n",
    "    train_y = train.values[:,-1]\n",
    "    valid_x = valid.values[:,:-1]\n",
    "    valid_y = valid.values[:,-1]\n",
    "    test_x = test.values[:,:-1]\n",
    "    test_y = test.values[:,-1]\n",
    "    RF_short = RandomForestClassifier(n_estimators = 15, max_depth =5)\n",
    "    RF_short.fit(train_x, train_y)\n",
    "    y_pred_rf_short = RF_short.predict(test_x)\n",
    "    y_pred_rf_short_prob = RF_short.predict_proba(test_x)\n",
    "    y_score_rf_l.append(RF_short.score(test_x,test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_score_rf_l.index(max(y_score_rf_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2009, 62)\n"
     ]
    }
   ],
   "source": [
    "index = 61\n",
    "hv = HashingVectorizer(n_features=index)\n",
    "trans = hv.transform(lyr2)\n",
    "# convert to dense matrix\n",
    "dense = trans.todense()\n",
    "dense = dense.tolist()\n",
    "for i in range(len(dense)):\n",
    "    dense[i].append(final_tag[i])\n",
    "final_df= pd.DataFrame.from_records(dense)\n",
    "print(final_df.shape),\n",
    "final_df[index] = final_df[index].map({'happy': 1.0, 'sad': 2.0,'energetic': 3.0, 'relax': 4.0})\n",
    "# split train_test file\n",
    "train = final_df.iloc[train_index]\n",
    "valid = final_df.iloc[valid_index]\n",
    "test = final_df.iloc[test_index]\n",
    "train_x = train.values[:,:-1]\n",
    "train_y = train.values[:,-1]\n",
    "valid_x = valid.values[:,:-1]\n",
    "valid_y = valid.values[:,-1]\n",
    "test_x = test.values[:,:-1]\n",
    "test_y = test.values[:,-1]\n",
    "RF_short = RandomForestClassifier(n_estimators = 15, max_depth =5)\n",
    "RF_short.fit(train_x, train_y)\n",
    "y_pred_rf_short = RF_short.predict(test_x)\n",
    "y_pred_rf_short_prob = RF_short.predict_proba(test_x)\n",
    "y_score = RF_short.score(test_x,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of Random Forest with HashingVectorizer after cross validation is: 0.5\n"
     ]
    }
   ],
   "source": [
    "print \"The accuracy of Random Forest with HashingVectorizer after cross validation is: \"+ str(y_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngIhIDVMQEBGpYf8f8aAM4tk6FfcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1179648d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_axis = np.arange(1,201,10)\n",
    "plt.plot(x_axis, y_score_rf_l)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
