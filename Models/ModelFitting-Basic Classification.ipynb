{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "happy = pd.read_csv('happy_counts.csv')\n",
    "relax = pd.read_csv('relax_counts.csv')\n",
    "energetic = pd.read_csv('energetic_counts.csv')\n",
    "sad = pd.read_csv('sad_counts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "happywords = happy.iloc[:,3:]\n",
    "sadwords = sad.iloc[:,3:]\n",
    "energeticwords = energetic.iloc[:,3:]\n",
    "relaxwords = relax.iloc[:,3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "happywords['giventag'] = 'happy'\n",
    "sadwords['giventag'] = 'sad'\n",
    "energeticwords['giventag'] = 'energetic'\n",
    "relaxwords['giventag'] = 'relax'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i</th>\n",
       "      <th>the</th>\n",
       "      <th>you</th>\n",
       "      <th>to</th>\n",
       "      <th>and</th>\n",
       "      <th>a</th>\n",
       "      <th>me</th>\n",
       "      <th>it</th>\n",
       "      <th>not</th>\n",
       "      <th>in</th>\n",
       "      <th>...</th>\n",
       "      <th>motivo</th>\n",
       "      <th>bake</th>\n",
       "      <th>insist</th>\n",
       "      <th>wel</th>\n",
       "      <th>santo</th>\n",
       "      <th>pe</th>\n",
       "      <th>gee</th>\n",
       "      <th>colleg</th>\n",
       "      <th>kad</th>\n",
       "      <th>giventag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>relax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>relax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>relax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>relax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39</td>\n",
       "      <td>17</td>\n",
       "      <td>21</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>relax</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    i  the  you  to  and   a  me  it  not  in    ...     motivo  bake  insist  \\\n",
       "0  28   15    2  12   22   2   2   4    2   1    ...          0     0       0   \n",
       "1  19   16    2   3    6  12   4   7    1   6    ...          0     0       0   \n",
       "2   2    9    8   7    2   1   0   7    3   0    ...          0     0       0   \n",
       "3   4    5    0   3    0   0   0   1    0   0    ...          0     0       0   \n",
       "4  39   17   21  11   10   6   2   6   12   7    ...          0     0       0   \n",
       "\n",
       "   wel  santo  pe  gee  colleg  kad  giventag  \n",
       "0    0      0   0    0       0    0     relax  \n",
       "1    0      0   0    0       0    0     relax  \n",
       "2    0      0   0    0       0    0     relax  \n",
       "3    0      0   0    0       0    0     relax  \n",
       "4    0      0   0    0       0    0     relax  \n",
       "\n",
       "[5 rows x 5001 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relaxwords.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For matching grouping songs into multiple tags, we start off with the common classification methods including Gaussian Naive Bayes, Support Vector Machine(SVM), and Random Forest. The focus of our work is to turn the original bag of words format of 5000 words into a collection of features with appropriate dimensionality and enough predictive power. We took two main steps of feature engineering:\n",
    "\n",
    "- Words selection\n",
    "\n",
    "- Dimensionality Reduction\n",
    "\n",
    "We first elaborate on word selection process. \n",
    "\n",
    "To ensure that we deal only with words that have strong predictive power, we consider removing the standard stop words from Python  library as well as the top common words across all targeted tags. We also attempted to convert the word counts into term frequency-inverse document frequency(tf-idf) so that we take into account the importance of a word to a song based on how many documents does the word show up in across entire corpus. We first run the models with all raw features with top 5000 word counts format with the three selected model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw features models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fullcounts = pd.concat([happywords, sadwords,energeticwords,relaxwords], axis = 0, ignore_index = True)\n",
    "trainfull = fullcounts.sample(frac = 0.75)\n",
    "testfull=fullcounts.drop(trainfull.index)\n",
    "\n",
    "x_full = fullcounts.drop(['giventag'], axis = 1)\n",
    "y_full = fullcounts['giventag']\n",
    "\n",
    "x_trainfull = trainfull.drop(['giventag'], axis = 1)\n",
    "y_trainfull = trainfull['giventag']\n",
    "x_testfull = testfull.drop(['giventag'], axis = 1)\n",
    "y_testfull = testfull['giventag']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gnb_full = GaussianNB()\n",
    "gnb_full.fit(x_trainfull, y_trainfull)\n",
    "y_pred_nb = gnb_full.predict(x_testfull)\n",
    "y_pred_nb_prob = gnb_full.predict_proba(x_testfull)\n",
    "#y_score_nb = gnb_full.score(x_testfull, y_testfull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21160558464223386"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y_pred_nb==y_testfull)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svm_full = SVC(kernel='rbf')\n",
    "svm_full.fit(x_trainfull, y_trainfull)\n",
    "y_pred_svm = svm_full.predict(x_testfull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42844677137870857"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y_pred_svm==y_testfull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': [5, 10, 15, 20, 25, 30, 35, 40], 'max_depth': [5, 10, 15, 20, 25, 30]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "                 'n_estimators': [5, 10, 15, 20, 25, 30, 35, 40],\n",
    "                 'max_depth': [5, 10, 15, 20, 25, 30]\n",
    "             }\n",
    "\n",
    "grid_clf = grid_search.GridSearchCV(RF, param_grid, cv=5)\n",
    "grid_clf.fit(x_trainfull, y_trainfull)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[mean: 0.36756, std: 0.00736, params: {'n_estimators': 5, 'max_depth': 5},\n",
       " mean: 0.37497, std: 0.00854, params: {'n_estimators': 10, 'max_depth': 5},\n",
       " mean: 0.37635, std: 0.00467, params: {'n_estimators': 15, 'max_depth': 5},\n",
       " mean: 0.37439, std: 0.00426, params: {'n_estimators': 20, 'max_depth': 5},\n",
       " mean: 0.37446, std: 0.00439, params: {'n_estimators': 25, 'max_depth': 5},\n",
       " mean: 0.37577, std: 0.00624, params: {'n_estimators': 30, 'max_depth': 5},\n",
       " mean: 0.37345, std: 0.00086, params: {'n_estimators': 35, 'max_depth': 5},\n",
       " mean: 0.37061, std: 0.00529, params: {'n_estimators': 40, 'max_depth': 5},\n",
       " mean: 0.32378, std: 0.01114, params: {'n_estimators': 5, 'max_depth': 10},\n",
       " mean: 0.33425, std: 0.00781, params: {'n_estimators': 10, 'max_depth': 10},\n",
       " mean: 0.33854, std: 0.00870, params: {'n_estimators': 15, 'max_depth': 10},\n",
       " mean: 0.34967, std: 0.00534, params: {'n_estimators': 20, 'max_depth': 10},\n",
       " mean: 0.35374, std: 0.00376, params: {'n_estimators': 25, 'max_depth': 10},\n",
       " mean: 0.35490, std: 0.00643, params: {'n_estimators': 30, 'max_depth': 10},\n",
       " mean: 0.36101, std: 0.00470, params: {'n_estimators': 35, 'max_depth': 10},\n",
       " mean: 0.35912, std: 0.00353, params: {'n_estimators': 40, 'max_depth': 10},\n",
       " mean: 0.27776, std: 0.00600, params: {'n_estimators': 5, 'max_depth': 15},\n",
       " mean: 0.29695, std: 0.00640, params: {'n_estimators': 10, 'max_depth': 15},\n",
       " mean: 0.30859, std: 0.00954, params: {'n_estimators': 15, 'max_depth': 15},\n",
       " mean: 0.30662, std: 0.00440, params: {'n_estimators': 20, 'max_depth': 15},\n",
       " mean: 0.31913, std: 0.00867, params: {'n_estimators': 25, 'max_depth': 15},\n",
       " mean: 0.31826, std: 0.00701, params: {'n_estimators': 30, 'max_depth': 15},\n",
       " mean: 0.32887, std: 0.01177, params: {'n_estimators': 35, 'max_depth': 15},\n",
       " mean: 0.32633, std: 0.00397, params: {'n_estimators': 40, 'max_depth': 15},\n",
       " mean: 0.27150, std: 0.00862, params: {'n_estimators': 5, 'max_depth': 20},\n",
       " mean: 0.28292, std: 0.01075, params: {'n_estimators': 10, 'max_depth': 20},\n",
       " mean: 0.29579, std: 0.00877, params: {'n_estimators': 15, 'max_depth': 20},\n",
       " mean: 0.30233, std: 0.00778, params: {'n_estimators': 20, 'max_depth': 20},\n",
       " mean: 0.30633, std: 0.00342, params: {'n_estimators': 25, 'max_depth': 20},\n",
       " mean: 0.31055, std: 0.00490, params: {'n_estimators': 30, 'max_depth': 20},\n",
       " mean: 0.32066, std: 0.00676, params: {'n_estimators': 35, 'max_depth': 20},\n",
       " mean: 0.31542, std: 0.00211, params: {'n_estimators': 40, 'max_depth': 20},\n",
       " mean: 0.25456, std: 0.00941, params: {'n_estimators': 5, 'max_depth': 25},\n",
       " mean: 0.27609, std: 0.00624, params: {'n_estimators': 10, 'max_depth': 25},\n",
       " mean: 0.29332, std: 0.01088, params: {'n_estimators': 15, 'max_depth': 25},\n",
       " mean: 0.29921, std: 0.00606, params: {'n_estimators': 20, 'max_depth': 25},\n",
       " mean: 0.30488, std: 0.00500, params: {'n_estimators': 25, 'max_depth': 25},\n",
       " mean: 0.31070, std: 0.00666, params: {'n_estimators': 30, 'max_depth': 25},\n",
       " mean: 0.31528, std: 0.00995, params: {'n_estimators': 35, 'max_depth': 25},\n",
       " mean: 0.31622, std: 0.00212, params: {'n_estimators': 40, 'max_depth': 25},\n",
       " mean: 0.25231, std: 0.00433, params: {'n_estimators': 5, 'max_depth': 30},\n",
       " mean: 0.27492, std: 0.00312, params: {'n_estimators': 10, 'max_depth': 30},\n",
       " mean: 0.28248, std: 0.00547, params: {'n_estimators': 15, 'max_depth': 30},\n",
       " mean: 0.29564, std: 0.00436, params: {'n_estimators': 20, 'max_depth': 30},\n",
       " mean: 0.30393, std: 0.00577, params: {'n_estimators': 25, 'max_depth': 30},\n",
       " mean: 0.30866, std: 0.00352, params: {'n_estimators': 30, 'max_depth': 30},\n",
       " mean: 0.31230, std: 0.00712, params: {'n_estimators': 35, 'max_depth': 30},\n",
       " mean: 0.31106, std: 0.00454, params: {'n_estimators': 40, 'max_depth': 30}]"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_clf.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 5, 'n_estimators': 15}"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RF_full = RandomForestClassifier(n_estimators = 15, max_depth =5)\n",
    "RF_full.fit(x_trainfull, y_trainfull)\n",
    "y_pred_rf = RF_full.predict(x_testfull)\n",
    "y_pred_rf_prob = RF_full.predict_proba(x_testfull)\n",
    "#y_score_rf = RF_full.score(x_testfull,y_testfull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36801919720767889"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y_pred_rf==y_testfull)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our attempts are made to try removing standard stop words, common words that appear in all four tags, and turning the word counts into term frequency - inverse document frequency format and apply the three selected models again to compare the performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Stop Words - models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4898\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>will</th>\n",
       "      <th>love</th>\n",
       "      <th>know</th>\n",
       "      <th>just</th>\n",
       "      <th>like</th>\n",
       "      <th>now</th>\n",
       "      <th>que</th>\n",
       "      <th>time</th>\n",
       "      <th>can</th>\n",
       "      <th>come</th>\n",
       "      <th>...</th>\n",
       "      <th>motivo</th>\n",
       "      <th>bake</th>\n",
       "      <th>insist</th>\n",
       "      <th>wel</th>\n",
       "      <th>santo</th>\n",
       "      <th>pe</th>\n",
       "      <th>gee</th>\n",
       "      <th>colleg</th>\n",
       "      <th>kad</th>\n",
       "      <th>giventag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>relax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>relax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>relax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>relax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>relax</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 4898 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   will  love  know  just  like  now  que  time  can  come    ...     motivo  \\\n",
       "0    13    11     0     1     1    0    0     6    0     4    ...          0   \n",
       "1     0     0     0     0     3    0    0     0    0     0    ...          0   \n",
       "2     0     0     1     3     0    0    0     0    6     0    ...          0   \n",
       "3     3     0     0     0     0    0    0     0    0     0    ...          0   \n",
       "4     4     0     4     1     0    2    0     0   10     0    ...          0   \n",
       "\n",
       "   bake  insist  wel  santo  pe  gee  colleg  kad  giventag  \n",
       "0     0       0    0      0   0    0       0    0     relax  \n",
       "1     0       0    0      0   0    0       0    0     relax  \n",
       "2     0       0    0      0   0    0       0    0     relax  \n",
       "3     0       0    0      0   0    0       0    0     relax  \n",
       "4     0       0    0      0   0    0       0    0     relax  \n",
       "\n",
       "[5 rows x 4898 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Try using Python package for stop words\n",
    "from stop_words import get_stop_words\n",
    "stop_words = get_stop_words('en')\n",
    "\n",
    "cols = [c for c in happywords.columns if c not in stop_words]\n",
    "print (len(cols))\n",
    "\n",
    "happywords_short = happywords[cols]\n",
    "sadwords_short = sadwords[cols]\n",
    "energeticwords_short = energeticwords[cols]\n",
    "relaxwords_short = relaxwords[cols]\n",
    "relaxwords_short.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts_short = pd.concat([happywords_short, sadwords_short,energeticwords_short,relaxwords_short], axis = 0, ignore_index = True)\n",
    "counts_short_tag = counts_short['giventag']\n",
    "counts_short_ct = counts_short.drop(['giventag'], axis = 1)\n",
    "\n",
    "train_short = counts_short.sample(frac = 0.75, random_state=100)\n",
    "test_short=counts_short.drop(train_short.index)\n",
    "\n",
    "x_short = counts_short.drop(['giventag'], axis = 1)\n",
    "y_short = counts_short['giventag']\n",
    "\n",
    "x_train_short = train_short.drop(['giventag'], axis = 1)\n",
    "y_train_short = train_short['giventag']\n",
    "x_test_short = test_short.drop(['giventag'], axis = 1)\n",
    "y_test_short = test_short['giventag']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gnb_short = GaussianNB()\n",
    "gnb_short.fit(x_train_short, y_train_short)\n",
    "y_pred_nb_short = gnb_short.predict(x_test_short)\n",
    "y_pred_nb_short_prob = gnb_short.predict_proba(x_test_short)\n",
    "#y_score_nb_short = gnb_short.score(x_test_short, y_test_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22709424083769633"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y_pred_nb_short==y_test_short)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X.shape[1] = 4897 should be equal to 5000, the number of features at training time",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-195-24a9e051f30c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msvm_short\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rbf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msvm_short\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_short\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_short\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0my_pred_svm_short\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvm_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_short\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#y_score_svm_short = svm_full.score(x_test_short, y_test_short)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py35/lib/python3.5/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    566\u001b[0m             \u001b[0mClass\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m         \"\"\"\n\u001b[0;32m--> 568\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseSVC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py35/lib/python3.5/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \"\"\"\n\u001b[0;32m--> 305\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_for_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sparse_predict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sparse\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dense_predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py35/lib/python3.5/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36m_validate_for_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    472\u001b[0m             raise ValueError(\"X.shape[1] = %d should be equal to %d, \"\n\u001b[1;32m    473\u001b[0m                              \u001b[0;34m\"the number of features at training time\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m                              (n_features, self.shape_fit_[1]))\n\u001b[0m\u001b[1;32m    475\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: X.shape[1] = 4897 should be equal to 5000, the number of features at training time"
     ]
    }
   ],
   "source": [
    "svm_short = SVC(kernel='rbf')\n",
    "svm_short.fit(x_train_short, y_train_short)\n",
    "y_pred_svm_short = svm_short.predict(x_test_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_svm_short = svm_short.predict(x_test_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41928446771378708"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y_pred_svm_short==y_test_short)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': [5, 10, 15, 20, 25, 30, 35, 40], 'max_depth': [5, 10, 15, 20, 25, 30]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "                 'n_estimators': [5, 10, 15, 20, 25, 30, 35, 40],\n",
    "                 'max_depth': [5, 10, 15, 20, 25, 30]\n",
    "             }\n",
    "\n",
    "grid_clf = grid_search.GridSearchCV(RF, param_grid, cv=5)\n",
    "grid_clf.fit(x_trainfull, y_trainfull)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 5, 'n_estimators': 15}"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RF = RandomForestClassifier()\n",
    "\n",
    "param_grid = {\n",
    "                 'n_estimators': [5, 10, 15, 20, 25, 30],\n",
    "                 'max_depth': [5, 10, 15, 20, 25]\n",
    "             }\n",
    "\n",
    "grid_clf = grid_search.GridSearchCV(RF, param_grid, cv=5)\n",
    "grid_clf.fit(x_train_short, y_train_short)\n",
    "\n",
    "grid_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RF_short = RandomForestClassifier(n_estimators = 15, max_depth =5)\n",
    "RF_short.fit(x_train_short, y_train_short)\n",
    "y_pred_rf_short = RF_short.predict(x_test_short)\n",
    "y_pred_rf_short_prob = RF_short.predict_proba(x_test_short)\n",
    "#y_score_rf_short = RF_short.score(x_test_short,y_test_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37172774869109948"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y_pred_rf_short==y_test_short)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Common Words - models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Add up the total occurrence for each word in each type\n",
    "total_happy = happywords_short.iloc[:,:4897].sum()\n",
    "total_sad = sadwords_short.iloc[:,:4897].sum()\n",
    "total_energetic = energeticwords_short.iloc[:,:4897].sum()\n",
    "total_relax = relaxwords_short.iloc[:,:4897].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ca', 'can', 'come', 'feel', 'get', 'go', 'got', 'just', 'know',\n",
       "       'let', 'like', 'love', 'make', 'never', 'now', 'oh', 'one', 'say',\n",
       "       'see', 'take', 'time', 'want', 'way', 'will'], dtype=object)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Sort by total number of occurences\n",
    "sorted_happy = total_happy.sort_values(ascending=False, inplace=False)\n",
    "sorted_sad = total_sad.sort_values(ascending=False, inplace=False)\n",
    "sorted_energetic = total_energetic.sort_values(ascending=False, inplace=False)\n",
    "sorted_relax = total_relax.sort_values(ascending=False, inplace=False)\n",
    "\n",
    "## Find the intersection of top 30 occurences in each category\n",
    "## Try using their intersection to be the stop list - words that do not add much meaning/value\n",
    "inter1 = np.intersect1d(sorted_happy.index.values[:30],sorted_sad.index.values[:30])\n",
    "inter2 = np.intersect1d(sorted_energetic.index.values[:30],sorted_relax.index.values[:30])\n",
    "stop_list = np.intersect1d(inter1, inter2)\n",
    "stop_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4874\n",
      "(5312, 4874)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/py35/lib/python3.5/site-packages/ipykernel/__main__.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "//anaconda/envs/py35/lib/python3.5/site-packages/ipykernel/__main__.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "//anaconda/envs/py35/lib/python3.5/site-packages/ipykernel/__main__.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "//anaconda/envs/py35/lib/python3.5/site-packages/ipykernel/__main__.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "cols = [c for c in happywords_short.columns if c not in stop_list]\n",
    "print (len(cols))\n",
    "\n",
    "happywords_final = happywords_short[cols]\n",
    "sadwords_final = sadwords_short[cols]\n",
    "energeticwords_final = energeticwords_short[cols]\n",
    "relaxwords_final = relaxwords_short[cols]\n",
    "\n",
    "happywords_final['giventag'] = 'happy'\n",
    "sadwords_final['giventag'] = 'sad'\n",
    "energeticwords_final['giventag'] = 'energetic'\n",
    "relaxwords_final['giventag'] = 'relax'\n",
    "\n",
    "print (happywords_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counts_nocommon = pd.concat([happywords_final, sadwords_final,energeticwords_final,relaxwords_final], axis = 0, ignore_index = True)\n",
    "train_nocommon = counts_nocommon.sample(frac = 0.75, random_state=100)\n",
    "test_nocommon=counts_nocommon.drop(train_nocommon.index)\n",
    "\n",
    "x_nocommon = counts_nocommon.drop(['giventag'], axis = 1)\n",
    "y_nocommon = counts_nocommon['giventag']\n",
    "\n",
    "x_train_nocommon = train_nocommon.drop(['giventag'], axis = 1)\n",
    "y_train_nocommon = train_nocommon['giventag']\n",
    "x_test_nocommon = test_nocommon.drop(['giventag'], axis = 1)\n",
    "y_test_nocommon = test_nocommon['giventag']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gnb_nocommon = GaussianNB()\n",
    "gnb_nocommon.fit(x_train_nocommon, y_train_nocommon)\n",
    "y_pred_nb_nocommon = gnb_nocommon.predict(x_test_nocommon)\n",
    "y_pred_nb_nocommon_prob = gnb_nocommon.predict_proba(x_test_nocommon)\n",
    "#y_score_nb_nocommon = gnb.score(x_test_nocommon, y_test_nocommon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22709424083769633"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y_pred_nb_nocommon == y_test_nocommon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svm_nocommon = SVC(kernel='rbf')\n",
    "svm_nocommon.fit(x_train_nocommon, y_train_nocommon)\n",
    "y_pred_svm_nocommon = svm_nocommon.predict(x_test_nocommon)\n",
    "#y_score_svm_nocommon = svm_nocommon.score(x_test_nocommon, y_test_nocommon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40990401396160558"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y_pred_svm_nocommon==y_test_nocommon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': [5, 10, 15, 20, 25, 30, 35, 40], 'max_depth': [5, 10, 15, 20, 25, 30]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "                 'n_estimators': [5, 10, 15, 20, 25, 30, 35, 40],\n",
    "                 'max_depth': [5, 10, 15, 20, 25, 30]\n",
    "             }\n",
    "\n",
    "grid_clf = grid_search.GridSearchCV(RF, param_grid, cv=5)\n",
    "grid_clf.fit(x_trainfull, y_trainfull)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 5, 'n_estimators': 15}"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RF_nocommon = RandomForestClassifier(n_estimators = 15, max_depth =5)\n",
    "RF_nocommon.fit(x_train_nocommon, y_train_nocommon)\n",
    "y_pred_rf_nocommon = RF_nocommon.predict(x_test_nocommon)\n",
    "y_pred_rf_nocommon_prob = RF_nocommon.predict_proba(x_test_nocommon)\n",
    "#y_score_rf_nocommon = RF.score(x_test_nocommon,y_test_nocommon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3763089005235602"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y_pred_rf_nocommon == y_test_nocommon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf-idf - account for relative frequency of words - models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer(norm='l2', smooth_idf=False, sublinear_tf=False,\n",
    "                 use_idf=True)\n",
    "transformer2 = TfidfTransformer(norm='l2', smooth_idf=False, sublinear_tf=True,\n",
    "                 use_idf=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>will</th>\n",
       "      <th>love</th>\n",
       "      <th>know</th>\n",
       "      <th>just</th>\n",
       "      <th>like</th>\n",
       "      <th>now</th>\n",
       "      <th>que</th>\n",
       "      <th>time</th>\n",
       "      <th>can</th>\n",
       "      <th>come</th>\n",
       "      <th>...</th>\n",
       "      <th>motivo</th>\n",
       "      <th>bake</th>\n",
       "      <th>insist</th>\n",
       "      <th>wel</th>\n",
       "      <th>santo</th>\n",
       "      <th>pe</th>\n",
       "      <th>gee</th>\n",
       "      <th>colleg</th>\n",
       "      <th>kad</th>\n",
       "      <th>giventag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.011388</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052688</td>\n",
       "      <td>0.027370</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.069047</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014595</td>\n",
       "      <td>0.029785</td>\n",
       "      <td>0.063891</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.067346</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.118384</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.047852</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.149618</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.038873</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029944</td>\n",
       "      <td>0.012847</td>\n",
       "      <td>0.013347</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006734</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028074</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.067695</td>\n",
       "      <td>0.019787</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019575</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 4898 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       will      love      know      just      like       now  que      time  \\\n",
       "0  0.011388  0.000000  0.000000  0.000000  0.052688  0.027370  0.0  0.000000   \n",
       "1  0.069047  0.000000  0.014595  0.029785  0.063891  0.000000  0.0  0.000000   \n",
       "2  0.118384  0.000000  0.000000  0.000000  0.000000  0.000000  0.0  0.047852   \n",
       "3  0.038873  0.000000  0.000000  0.029944  0.012847  0.013347  0.0  0.006734   \n",
       "4  0.067695  0.019787  0.000000  0.000000  0.019575  0.000000  0.0  0.000000   \n",
       "\n",
       "        can      come    ...     motivo  bake  insist  wel  santo   pe  gee  \\\n",
       "0  0.000000  0.000000    ...        0.0   0.0     0.0  0.0    0.0  0.0  0.0   \n",
       "1  0.067346  0.000000    ...        0.0   0.0     0.0  0.0    0.0  0.0  0.0   \n",
       "2  0.000000  0.149618    ...        0.0   0.0     0.0  0.0    0.0  0.0  0.0   \n",
       "3  0.000000  0.028074    ...        0.0   0.0     0.0  0.0    0.0  0.0  0.0   \n",
       "4  0.000000  0.000000    ...        0.0   0.0     0.0  0.0    0.0  0.0  0.0   \n",
       "\n",
       "   colleg  kad  giventag  \n",
       "0     0.0  0.0     happy  \n",
       "1     0.0  0.0     happy  \n",
       "2     0.0  0.0     happy  \n",
       "3     0.0  0.0     happy  \n",
       "4     0.0  0.0     happy  \n",
       "\n",
       "[5 rows x 4898 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = transformer.fit_transform(counts_short_ct)\n",
    "tfidf = pd.DataFrame(tfidf.toarray())\n",
    "#counts_short_tag = counts_short_tag.to_frame()\n",
    "counts_short_tag.index = tfidf.index.values\n",
    "tfidf_label = pd.concat([tfidf,counts_short_tag], axis = 1)\n",
    "tfidf_label.columns = counts_short.columns.values\n",
    "tfidf_label.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfidf_train = tfidf_label.sample(frac = 0.75, random_state=100)\n",
    "tfidf_test=tfidf_label.drop(tfidf_train.index)\n",
    "\n",
    "tfidf_x = tfidf_label.drop(['giventag'], axis = 1)\n",
    "tfidf_y = tfidf_label['giventag']\n",
    "\n",
    "tfidf_x_train = tfidf_train.drop(['giventag'], axis = 1)\n",
    "tfidf_y_train = tfidf_train['giventag']\n",
    "tfidf_x_test = tfidf_test.drop(['giventag'], axis = 1)\n",
    "tfidf_y_test = tfidf_test['giventag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_gnb = GaussianNB()\n",
    "tfidf_gnb.fit(tfidf_x_train, tfidf_y_train)\n",
    "tfidf_y_pred_nb = tfidf_gnb.predict(tfidf_x_test)\n",
    "tfidf_y_pred_nb_prob = tfidf_gnb.predict_proba(tfidf_x_test)\n",
    "#y_score_nb_nocommon = gnb.score(x_test_nocommon, y_test_nocommon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21836823734729494"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(tfidf_y_pred_nb == tfidf_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-149-06d85bcd2bf1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msvm_nocommon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rbf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msvm_nocommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_nocommon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_nocommon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0my_pred_svm_nocommon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvm_nocommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_nocommon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_score_svm_nocommon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvm_nocommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_nocommon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_nocommon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py35/lib/python3.5/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m         \u001b[0;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py35/lib/python3.5/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36m_dense_fit\u001b[0;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[1;32m    249\u001b[0m                 \u001b[0mcache_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoef0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m                 \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m                 max_iter=self.max_iter, random_seed=random_seed)\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_warn_from_fit_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tfidf_svm = SVC(kernel='rbf')\n",
    "tfidf_svm.fit(tfidf_x_train, tfidf_y_train)\n",
    "tfidf_y_pred_svm = tfidf_svm.predict(tfidf_x_test)\n",
    "tfidf_y_score_svm = tfidf_svm.score(tfidf_x_test, tfidf_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.mean(tfidf_y_pred_svm==tfidf_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': [5, 10, 15, 20, 25, 30, 35, 40], 'max_depth': [5, 10, 15, 20, 25, 30]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "                 'n_estimators': [5, 10, 15, 20, 25, 30, 35, 40],\n",
    "                 'max_depth': [5, 10, 15, 20, 25, 30]\n",
    "             }\n",
    "\n",
    "grid_clf = grid_search.GridSearchCV(RF, param_grid, cv=5)\n",
    "grid_clf.fit(x_trainfull, y_trainfull)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 5, 'n_estimators': 15}"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf_RF = RandomForestClassifier(n_estimators = 15, max_depth =5)\n",
    "tfidf_RF.fit(tfidf_x_train, tfidf_y_train)\n",
    "tfidf_y_pred_rf = tfidf_RF.predict(tfidf_x_test)\n",
    "tfidf_y_pred_rf_prob = tfidf_RF.predict_proba(tfidf_x_test)\n",
    "#y_score_rf_nocommon = RF.score(x_test_nocommon,y_test_nocommon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3736910994764398"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(tfidf_y_pred_rf == tfidf_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply PCA - stop word removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9049027665447078"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=800)\n",
    "pca.fit(x_short)\n",
    "np.sum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_reduced = pca.transform(x_short)\n",
    "x_reduced = pd.DataFrame(x_reduced)\n",
    "y_short = y_short.to_frame()\n",
    "y_short.index = x_reduced.index.values\n",
    "xy_reduced = pd.concat([x_reduced, y_short], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_reduced = xy_reduced.sample(frac = 0.75, random_state=100)\n",
    "test_reduced=xy_reduced.drop(train_reduced.index)\n",
    "\n",
    "x_reduced = xy_reduced.drop(['giventag'], axis = 1)\n",
    "y_reduced = xy_reduced['giventag']\n",
    "\n",
    "x_train_reduced = train_reduced.drop(['giventag'], axis = 1)\n",
    "y_train_reduced = train_reduced['giventag']\n",
    "x_test_reduced = test_reduced.drop(['giventag'], axis = 1)\n",
    "y_test_reduced = test_reduced['giventag']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb_reduced = GaussianNB()\n",
    "nb_reduced.fit(x_train_reduced, y_train_reduced)\n",
    "y_pred_nb_reduced = nb_reduced.predict(x_test_reduced)\n",
    "y_pred_nb_reduced_prob = nb_reduced.predict_proba(x_test_reduced)\n",
    "#y_score_nb_nocommon = gnb.score(x_test_nocommon, y_test_nocommon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34969458987783597"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y_pred_nb_reduced==y_test_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svm_reduced = SVC(kernel='rbf')\n",
    "svm_reduced.fit(x_train_reduced, y_train_reduced)\n",
    "y_pred_svm_reduced = svm_reduced.predict(x_test_reduced)\n",
    "#y_score_svm_reduced = svm_reduced.score(x_test_reduced, y_test_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41383071553228623"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y_pred_svm_reduced==y_test_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': [5, 10, 15, 20, 25, 30, 35, 40], 'max_depth': [5, 10, 15, 20, 25, 30]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "                 'n_estimators': [5, 10, 15, 20, 25, 30, 35, 40],\n",
    "                 'max_depth': [5, 10, 15, 20, 25, 30]\n",
    "             }\n",
    "\n",
    "grid_clf = grid_search.GridSearchCV(RF, param_grid, cv=5)\n",
    "grid_clf.fit(x_trainfull, y_trainfull)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 5, 'n_estimators': 15}"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RF_reduced = RandomForestClassifier(n_estimators = 15, max_depth =5)\n",
    "RF_reduced.fit(x_train_reduced, y_train_reduced)\n",
    "y_pred_rf_reduced = RF_reduced.predict(x_test_reduced)\n",
    "y_pred_rf_reduced_prob = RF_reduced.predict_proba(x_test_reduced)\n",
    "#y_score_rf_nocommon = RF.score(x_test_nocommon,y_test_nocommon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39419720767888305"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y_pred_rf_reduced==y_test_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf-idf then apply PCA - stop word removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71698660295419991"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_pca1 = PCA(n_components=800)\n",
    "tfidf_pca1.fit(tfidf_x)\n",
    "np.sum(tfidf_pca1.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89454253341783152"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_pca = PCA(n_components=1800)\n",
    "tfidf_pca.fit(tfidf_x)\n",
    "np.sum(tfidf_pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_x_reduced = tfidf_pca1.transform(tfidf_x)\n",
    "tfidf_x_reduced = pd.DataFrame(tfidf_x_reduced)\n",
    "tfidf_y_reduced = tfidf_y.to_frame()\n",
    "tfidf_y_reduced.index = tfidf_x_reduced.index.values\n",
    "tfidf_xy_reduced = pd.concat([tfidf_x_reduced, tfidf_y_reduced], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_train_reduced = tfidf_xy_reduced.sample(frac = 0.75, random_state=100)\n",
    "tfidf_test_reduced=tfidf_xy_reduced.drop(tfidf_train_reduced.index)\n",
    "\n",
    "tfidf_x_reduced = tfidf_xy_reduced.drop(['giventag'], axis = 1)\n",
    "tfidf_y_reduced = tfidf_xy_reduced['giventag']\n",
    "\n",
    "tfidf_x_train_reduced = tfidf_train_reduced.drop(['giventag'], axis = 1)\n",
    "tfidf_y_train_reduced = tfidf_train_reduced['giventag']\n",
    "tfidf_x_test_reduced = tfidf_test_reduced.drop(['giventag'], axis = 1)\n",
    "tfidf_y_test_reduced = tfidf_test_reduced['giventag']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4584,)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_y_pred_nb_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4584,)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_y_test_reduced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_nb_reduced = GaussianNB()\n",
    "tfidf_nb_reduced.fit(tfidf_x_train_reduced, tfidf_y_train_reduced)\n",
    "tfidf_y_pred_nb_reduced_prob = tfidf_nb_reduced.predict_proba(tfidf_x_test_reduced)\n",
    "tfidf_y_pred_nb_reduced = tfidf_nb_reduced.predict(tfidf_x_test_reduced)\n",
    "#tfidf_nb_reduced.score(tfidf_x_test_reduced, tfidf_y_test_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31173647469458987"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(tfidf_y_pred_nb_reduced==tfidf_y_test_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_svm_reduced = SVC(kernel='rbf')\n",
    "tfidf_svm_reduced.fit(tfidf_x_train_reduced, tfidf_y_train_reduced)\n",
    "tfidf_y_pred_svm_reduced = tfidf_svm_reduced.predict(tfidf_x_test_reduced)\n",
    "#y_predprob1 = clffull.predict_proba(x_testfull)\n",
    "#tfidf_svm_reduced.score(tfidf_x_test_reduced, y_testfull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.mean(tfidf_y_pred_svm_reduced==tfidf_y_test_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': [5, 10, 15, 20, 25, 30, 35, 40], 'max_depth': [5, 10, 15, 20, 25, 30]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "                 'n_estimators': [5, 10, 15, 20, 25, 30, 35, 40],\n",
    "                 'max_depth': [5, 10, 15, 20, 25, 30]\n",
    "             }\n",
    "\n",
    "grid_clf = grid_search.GridSearchCV(RF, param_grid, cv=5)\n",
    "grid_clf.fit(x_trainfull, y_trainfull)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 5, 'n_estimators': 15}"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_rf_reduced = RandomForestClassifier(n_estimators = 15, max_depth =5)\n",
    "tfidf_rf_reduced.fit(tfidf_x_train_reduced, tfidf_y_train_reduced)\n",
    "tfidf_y_pred_rf_reduced = tfidf_rf_reduced.predict(tfidf_x_test_reduced)\n",
    "tfidf_ypred_rf_reduced_prob = tfidf_rf_reduced.predict_proba(tfidf_x_test_reduced)\n",
    "#score_rf = tfidf_rf_reduced.score(x_testfull,y_testfull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.375"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(tfidf_y_pred_rf_reduced==tfidf_y_test_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90404657432890489"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsvd = TruncatedSVD(n_components=800)\n",
    "tsvd.fit(x_short)\n",
    "np.sum(tsvd.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_reduced_tsvd = tsvd.transform(x_short)\n",
    "x_reduced_tsvd = pd.DataFrame(x_reduced_tsvd)\n",
    "#y_short = y_short.to_frame()\n",
    "y_short.index = x_reduced_tsvd.index.values\n",
    "xy_reduced_tsvd = pd.concat([x_reduced_tsvd, y_short], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_reduced_tsvd = xy_reduced_tsvd.sample(frac = 0.75, random_state=100)\n",
    "test_reduced_tsvd=xy_reduced_tsvd.drop(train_reduced_tsvd.index)\n",
    "\n",
    "x_reduced_tsvd = xy_reduced_tsvd.drop(['giventag'], axis = 1)\n",
    "y_reduced_tsvd = xy_reduced_tsvd['giventag']\n",
    "\n",
    "x_train_reduced_tsvd = train_reduced_tsvd.drop(['giventag'], axis = 1)\n",
    "y_train_reduced_tsvd = train_reduced_tsvd['giventag']\n",
    "x_test_reduced_tsvd = test_reduced_tsvd.drop(['giventag'], axis = 1)\n",
    "y_test_reduced_tsvd = test_reduced_tsvd['giventag']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_reduced_tsvd = GaussianNB()\n",
    "nb_reduced_tsvd.fit(x_train_reduced_tsvd, y_train_reduced_tsvd)\n",
    "y_pred_nb_reduced_tsvd = nb_reduced_tsvd.predict(x_test_reduced_tsvd)\n",
    "y_pred_nb_reduced_tsvd_prob = nb_reduced_tsvd.predict_proba(x_test_reduced_tsvd)\n",
    "#y_score_nb_nocommon = gnb.score(x_test_nocommon, y_test_nocommon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3505671902268761"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y_pred_nb_reduced_tsvd==y_test_reduced_tsvd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svm_reduced_tsvd = SVC(kernel='rbf')\n",
    "svm_reduced_tsvd.fit(x_train_reduced_tsvd, y_train_reduced_tsvd)\n",
    "y_pred_svm_reduced_tsvd = svm_reduced_tsvd.predict(x_test_reduced_tsvd)\n",
    "#y_score_svm_reduced_tsvd = svm_reduced.score(x_test_reduced, y_test_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41404886561954624"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y_pred_svm_reduced_tsvd==y_test_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': [5, 10, 15, 20, 25, 30, 35, 40], 'max_depth': [5, 10, 15, 20, 25, 30]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "                 'n_estimators': [5, 10, 15, 20, 25, 30, 35, 40],\n",
    "                 'max_depth': [5, 10, 15, 20, 25, 30]\n",
    "             }\n",
    "\n",
    "grid_clf = grid_search.GridSearchCV(RF, param_grid, cv=5)\n",
    "grid_clf.fit(x_trainfull, y_trainfull)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 5, 'n_estimators': 15}"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RF_reduced_tsvd = RandomForestClassifier(n_estimators = 15, max_depth =5)\n",
    "RF_reduced_tsvd.fit(x_train_reduced_tsvd, y_train_reduced_tsvd)\n",
    "y_pred_rf_reduced_tsvd = RF_reduced_tsvd.predict(x_test_reduced_tsvd)\n",
    "y_pred_rf_reduced_tsvd_prob = RF_reduced_tsvd.predict_proba(x_test_reduced_tsvd)\n",
    "#y_score_rf_nocommon = RF.score(x_test_nocommon,y_test_nocommon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39703315881326351"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y_pred_rf_reduced==y_test_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf-idf then apply TruncatedSVD - stop word removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71434522174070392"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_tsvd = TruncatedSVD(n_components=800)\n",
    "tfidf_tsvd.fit(tfidf_x)\n",
    "np.sum(tfidf_tsvd.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf_x_reduced_tsvd = tfidf_tsvd.transform(tfidf_x)\n",
    "tfidf_x_reduced_tsvd = pd.DataFrame(tfidf_x_reduced_tsvd)\n",
    "tfidf_y_reduced_tsvd = tfidf_y.to_frame()\n",
    "tfidf_y_reduced_tsvd.index = tfidf_x_reduced_tsvd.index.values\n",
    "tfidf_xy_reduced_tsvd = pd.concat([tfidf_x_reduced_tsvd, tfidf_y_reduced_tsvd], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_train_reduced_tsvd = tfidf_xy_reduced_tsvd.sample(frac = 0.75, random_state=100)\n",
    "tfidf_test_reduced_tsvd=tfidf_xy_reduced_tsvd.drop(tfidf_train_reduced_tsvd.index)\n",
    "\n",
    "tfidf_x_reduced_tsvd = tfidf_xy_reduced_tsvd.drop(['giventag'], axis = 1)\n",
    "tfidf_y_reduced_tsvd = tfidf_xy_reduced_tsvd['giventag']\n",
    "\n",
    "tfidf_x_train_reduced_tsvd = tfidf_train_reduced_tsvd.drop(['giventag'], axis = 1)\n",
    "tfidf_y_train_reduced_tsvd = tfidf_train_reduced_tsvd['giventag']\n",
    "tfidf_x_test_reduced_tsvd = tfidf_test_reduced_tsvd.drop(['giventag'], axis = 1)\n",
    "tfidf_y_test_reduced_tsvd = tfidf_test_reduced_tsvd['giventag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_nb_reduced_tsvd = GaussianNB()\n",
    "tfidf_nb_reduced_tsvd.fit(tfidf_x_train_reduced_tsvd, tfidf_y_train_reduced_tsvd)\n",
    "tfidf_y_pred_nb_reduced_tsvd_prob = tfidf_nb_reduced_tsvd.predict_proba(tfidf_x_test_reduced_tsvd)\n",
    "tfidf_y_pred_nb_reduced_tsvd = tfidf_nb_reduced_tsvd.predict(tfidf_x_test_reduced_tsvd)\n",
    "#tfidf_nb_reduced.score(tfidf_x_test_reduced, tfidf_y_test_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31849912739965097"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(tfidf_y_pred_nb_reduced_tsvd==tfidf_y_test_reduced_tsvd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_svm_reduced_tsvd = SVC(kernel='rbf')\n",
    "tfidf_svm_reduced_tsvd.fit(tfidf_x_train_reduced_tsvd, tfidf_y_train_reduced_tsvd)\n",
    "tfidf_y_pred_svm_reduced_tsvd = tfidf_svm_reduced_tsvd.predict(tfidf_x_test_reduced_tsvd)\n",
    "#y_predprob1 = clffull.predict_proba(x_testfull)\n",
    "#tfidf_svm_reduced.score(tfidf_x_test_reduced, y_testfull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34162303664921467"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(tfidf_y_pred_svm_reduced_tsvd==tfidf_y_test_reduced_tsvd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': [5, 10, 15, 20, 25, 30, 35, 40], 'max_depth': [5, 10, 15, 20, 25, 30]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "                 'n_estimators': [5, 10, 15, 20, 25, 30, 35, 40],\n",
    "                 'max_depth': [5, 10, 15, 20, 25, 30]\n",
    "             }\n",
    "\n",
    "grid_clf = grid_search.GridSearchCV(RF, param_grid, cv=5)\n",
    "grid_clf.fit(x_trainfull, y_trainfull)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 5, 'n_estimators': 15}"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_rf_reduced_tsvd = RandomForestClassifier(n_estimators = 15, max_depth =5)\n",
    "tfidf_rf_reduced_tsvd.fit(tfidf_x_train_reduced_tsvd, tfidf_y_train_reduced_tsvd)\n",
    "tfidf_y_pred_rf_reduced_tsvd = tfidf_rf_reduced_tsvd.predict(tfidf_x_test_reduced_tsvd)\n",
    "tfidf_ypred_rf_reduced_tsvd_prob = tfidf_rf_reduced_tsvd.predict_proba(tfidf_x_test_reduced_tsvd)\n",
    "#score_rf = tfidf_rf_reduced.score(x_testfull,y_testfull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39027050610820246"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(tfidf_y_pred_rf_reduced_tsvd==tfidf_y_test_reduced_tsvd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Preliminary Model Analysis and implications: \n",
    "Noticing that we have less than 50% accuracy with all above methods we tried, we want to dig into the reasons behind. By performing only classification decisions on the data that we are certain about based on the selected models for approximately 51% of the training and testing data, we only increase the classification accuracy by 3%. This led us to think that the reasons behind the unsatisfactory results may be from the inappropriate choice of targets. \n",
    "\n",
    "By running for individual binary classification for each individual target tags, we notice a significant increase in the prediction accuracy from the same reduced lyrics dataset to of at least 65%. This shed light to our guess of the problem of the target tags not being mutually exclusive. More specifically, we reconsider our choice of target groups and continue to explore natural separation in topics with further unsupervised learning methods.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only classify if max prob - 2nd large prob > 0.07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probs = pd.read_csv('for_draw1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probs1_val = probs.iloc[:,1:5]\n",
    "probs1_true = probs.iloc[:,5]\n",
    "probs1_pred = probs.iloc[:,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import heapq\n",
    "top2 = [heapq.nlargest(2,probs1_val.iloc[i,:]) for i in range(probs1_val.shape[0])]\n",
    "diff = np.array([abs(x[0]-x[1]) for x in top2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.54183266932270913"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_bool = (diff>=0.07)\n",
    "#Proportion of data points that we are able to classify with the threshold\n",
    "np.mean(pred_bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "originally prediction accuracy: 0.488047808765\n",
      "new prediciton accuracy: 0.511029411765\n"
     ]
    }
   ],
   "source": [
    "pred_bool = (diff>=0.07)\n",
    "#prediction accuracy\n",
    "print (\"originally prediction accuracy:\", np.mean((probs1_pred==probs1_true)))\n",
    "print (\"new prediciton accuracy:\", np.mean((probs1_pred==probs1_true)[pred_bool]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to look at how much accuracy we achieve within each category specifically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conf_matrix(pred, true):\n",
    "    overall = np.mean(pred==true)\n",
    "    happy = np.mean((pred==true)[(true=='happy') | (true==1)])\n",
    "    sad = np.mean((pred==true)[(true=='sad') | (true==2)])\n",
    "    energetic = np.mean((pred==true)[(true=='energetic') | (true==3)])\n",
    "    relaxing = np.mean((pred==true)[(true=='relaxing') | (true==4)])\n",
    "    print (\"overall: \",overall)\n",
    "    print (\"happy: \",happy)\n",
    "    print (\"sad: \",sad)\n",
    "    print (\"energetic: \",energetic)\n",
    "    print (\"relaxing: \",relaxing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "probs1_pred = np.array(probs1_pred)\n",
    "probs1_true = np.array(probs1_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall:  0.488047808765\n",
      "happy:  0.541176470588\n",
      "sad:  0.790322580645\n",
      "energetic:  0.027027027027\n",
      "relaxing:  0.0555555555556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/py35/lib/python3.5/site-packages/ipykernel/__main__.py:3: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  app.launch_new_instance()\n",
      "//anaconda/envs/py35/lib/python3.5/site-packages/ipykernel/__main__.py:4: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "//anaconda/envs/py35/lib/python3.5/site-packages/ipykernel/__main__.py:5: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "//anaconda/envs/py35/lib/python3.5/site-packages/ipykernel/__main__.py:6: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n"
     ]
    }
   ],
   "source": [
    "conf_matrix(probs1_pred,probs1_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': [5, 10, 15, 20, 25, 30, 35, 40], 'max_depth': [5, 10, 15, 20, 25, 30]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "                 'n_estimators': [5, 10, 15, 20, 25, 30, 35, 40],\n",
    "                 'max_depth': [5, 10, 15, 20, 25, 30]\n",
    "             }\n",
    "\n",
    "grid_clf = grid_search.GridSearchCV(RF, param_grid, cv=5)\n",
    "grid_clf.fit(x_trainfull, y_trainfull)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 5, 'n_estimators': 15}"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[mean: 0.36756, std: 0.00736, params: {'n_estimators': 5, 'max_depth': 5},\n",
       " mean: 0.37497, std: 0.00854, params: {'n_estimators': 10, 'max_depth': 5},\n",
       " mean: 0.37635, std: 0.00467, params: {'n_estimators': 15, 'max_depth': 5},\n",
       " mean: 0.37439, std: 0.00426, params: {'n_estimators': 20, 'max_depth': 5},\n",
       " mean: 0.37446, std: 0.00439, params: {'n_estimators': 25, 'max_depth': 5},\n",
       " mean: 0.37577, std: 0.00624, params: {'n_estimators': 30, 'max_depth': 5},\n",
       " mean: 0.37345, std: 0.00086, params: {'n_estimators': 35, 'max_depth': 5},\n",
       " mean: 0.37061, std: 0.00529, params: {'n_estimators': 40, 'max_depth': 5},\n",
       " mean: 0.32378, std: 0.01114, params: {'n_estimators': 5, 'max_depth': 10},\n",
       " mean: 0.33425, std: 0.00781, params: {'n_estimators': 10, 'max_depth': 10},\n",
       " mean: 0.33854, std: 0.00870, params: {'n_estimators': 15, 'max_depth': 10},\n",
       " mean: 0.34967, std: 0.00534, params: {'n_estimators': 20, 'max_depth': 10},\n",
       " mean: 0.35374, std: 0.00376, params: {'n_estimators': 25, 'max_depth': 10},\n",
       " mean: 0.35490, std: 0.00643, params: {'n_estimators': 30, 'max_depth': 10},\n",
       " mean: 0.36101, std: 0.00470, params: {'n_estimators': 35, 'max_depth': 10},\n",
       " mean: 0.35912, std: 0.00353, params: {'n_estimators': 40, 'max_depth': 10},\n",
       " mean: 0.27776, std: 0.00600, params: {'n_estimators': 5, 'max_depth': 15},\n",
       " mean: 0.29695, std: 0.00640, params: {'n_estimators': 10, 'max_depth': 15},\n",
       " mean: 0.30859, std: 0.00954, params: {'n_estimators': 15, 'max_depth': 15},\n",
       " mean: 0.30662, std: 0.00440, params: {'n_estimators': 20, 'max_depth': 15},\n",
       " mean: 0.31913, std: 0.00867, params: {'n_estimators': 25, 'max_depth': 15},\n",
       " mean: 0.31826, std: 0.00701, params: {'n_estimators': 30, 'max_depth': 15},\n",
       " mean: 0.32887, std: 0.01177, params: {'n_estimators': 35, 'max_depth': 15},\n",
       " mean: 0.32633, std: 0.00397, params: {'n_estimators': 40, 'max_depth': 15},\n",
       " mean: 0.27150, std: 0.00862, params: {'n_estimators': 5, 'max_depth': 20},\n",
       " mean: 0.28292, std: 0.01075, params: {'n_estimators': 10, 'max_depth': 20},\n",
       " mean: 0.29579, std: 0.00877, params: {'n_estimators': 15, 'max_depth': 20},\n",
       " mean: 0.30233, std: 0.00778, params: {'n_estimators': 20, 'max_depth': 20},\n",
       " mean: 0.30633, std: 0.00342, params: {'n_estimators': 25, 'max_depth': 20},\n",
       " mean: 0.31055, std: 0.00490, params: {'n_estimators': 30, 'max_depth': 20},\n",
       " mean: 0.32066, std: 0.00676, params: {'n_estimators': 35, 'max_depth': 20},\n",
       " mean: 0.31542, std: 0.00211, params: {'n_estimators': 40, 'max_depth': 20},\n",
       " mean: 0.25456, std: 0.00941, params: {'n_estimators': 5, 'max_depth': 25},\n",
       " mean: 0.27609, std: 0.00624, params: {'n_estimators': 10, 'max_depth': 25},\n",
       " mean: 0.29332, std: 0.01088, params: {'n_estimators': 15, 'max_depth': 25},\n",
       " mean: 0.29921, std: 0.00606, params: {'n_estimators': 20, 'max_depth': 25},\n",
       " mean: 0.30488, std: 0.00500, params: {'n_estimators': 25, 'max_depth': 25},\n",
       " mean: 0.31070, std: 0.00666, params: {'n_estimators': 30, 'max_depth': 25},\n",
       " mean: 0.31528, std: 0.00995, params: {'n_estimators': 35, 'max_depth': 25},\n",
       " mean: 0.31622, std: 0.00212, params: {'n_estimators': 40, 'max_depth': 25},\n",
       " mean: 0.25231, std: 0.00433, params: {'n_estimators': 5, 'max_depth': 30},\n",
       " mean: 0.27492, std: 0.00312, params: {'n_estimators': 10, 'max_depth': 30},\n",
       " mean: 0.28248, std: 0.00547, params: {'n_estimators': 15, 'max_depth': 30},\n",
       " mean: 0.29564, std: 0.00436, params: {'n_estimators': 20, 'max_depth': 30},\n",
       " mean: 0.30393, std: 0.00577, params: {'n_estimators': 25, 'max_depth': 30},\n",
       " mean: 0.30866, std: 0.00352, params: {'n_estimators': 30, 'max_depth': 30},\n",
       " mean: 0.31230, std: 0.00712, params: {'n_estimators': 35, 'max_depth': 30},\n",
       " mean: 0.31106, std: 0.00454, params: {'n_estimators': 40, 'max_depth': 30}]"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_clf.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RF = RandomForestClassifier(n_estimators = 15, max_depth =5)\n",
    "RF.fit(x_trainfull, y_trainfull)\n",
    "ypred_rf = RF.predict(x_testfull)\n",
    "ypred_rf_prob = RF.predict_proba(x_testfull)\n",
    "score_rf = RF.score(x_testfull,y_testfull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ypred_rf_prob_df = pd.DataFrame(ypred_rf_prob)\n",
    "ypred_rf_prob_df.index = y_testfull.index.values\n",
    "ypred_rf_prob_df_tag = pd.concat([ypred_rf_prob_df, y_testfull], axis=1)\n",
    "ypred_rf_prob_df_tag.to_csv('rf_probs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
